{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Martin_Lukan_Sentiment_Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlukan/GDA3B2021/blob/main/Martin_Lukan_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbpyaJZEydQm"
      },
      "source": [
        "1.\tImport this dataset of tweets into a DataFrame.\n",
        "2.\tKeep only positive and negative tweets (so you exclude neutrals). What is the percentage of positive/negative tweets?\n",
        "\n",
        "url='https://github.com/murpi/wilddata/raw/master/quests/tweets.zip'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyH6iyd0ydQu"
      },
      "source": [
        "url=\"https://github.com/murpi/wilddata/raw/master/quests/tweets.zip\"\n",
        "import requests, zipfile, io\n",
        "import os\n",
        "r = requests.get(url)\n",
        "\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "filepath=os.getcwd()+\"/files/\"\n",
        "z.extractall(filepath)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyF12O_AydQw",
        "outputId": "8096d0c3-0f27-435a-9e1d-f788d99eeff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "import pandas as pd\n",
        "filename= z.filelist[0].filename\n",
        "tweetsdf=pd.read_csv(filepath+filename)\n",
        "print(len(tweetsdf))\n",
        "tweetsdf=tweetsdf[tweetsdf['sentiment']!='neutral']\n",
        "tweetsdf.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16363</td>\n",
              "      <td>16363</td>\n",
              "      <td>16363</td>\n",
              "      <td>16363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>16363</td>\n",
              "      <td>16363</td>\n",
              "      <td>11377</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>db7dc72889</td>\n",
              "      <td>g`mornin</td>\n",
              "      <td>good</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>199</td>\n",
              "      <td>8582</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            textID       text selected_text sentiment\n",
              "count        16363      16363         16363     16363\n",
              "unique       16363      16363         11377         2\n",
              "top     db7dc72889   g`mornin          good  positive\n",
              "freq             1          1           199      8582"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo6qfvBEydQx",
        "outputId": "f0c0cbd0-5681-4a93-9306-150f23646037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tweetsdf['sentiment'].value_counts(normalize=True)*100"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    52.447595\n",
              "negative    47.552405\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gedBl8lqydQy"
      },
      "source": [
        "3.\tRetrieve the list of English stopwords from NLTK, and copy it into a stopwordsenglish list.\n",
        "4.\tCreate a lemma function that takes a text (str) as a parameter and returns a text (str) of tokens after applying a stemmer or a lemmatizer, separated by spaces.\n",
        "5.\tApply this function lemma to the text column of your DataFrame. Store the result in a new lemma column of the DataFrame. (The processing can take 2 or 3 minutes).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roWWJkYwydQy",
        "outputId": "9211dd94-f018-414c-8e9d-63c492cf6f59"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords\n",
        "stopwordsenglish=stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemma(sentence):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in sentence.split()])\n",
        "\n",
        "tweetsdf['lemma']=tweetsdf['text'].apply(lambda x: lemma(x))\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/A19893678/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/A19893678/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhWl2JD4ydQy",
        "outputId": "fc9af3ee-7d0a-4477-e68a-b9930bb8d3a1"
      },
      "source": [
        "tweetsdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>my bos is bullying me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6e0c6d75b1</td>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>fun</td>\n",
              "      <td>positive</td>\n",
              "      <td>2am feeding for the baby are fun when he is al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27475</th>\n",
              "      <td>b78ec00df5</td>\n",
              "      <td>enjoy ur night</td>\n",
              "      <td>enjoy</td>\n",
              "      <td>positive</td>\n",
              "      <td>enjoy ur night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27476</th>\n",
              "      <td>4eac33d1c0</td>\n",
              "      <td>wish we could come see u on Denver  husband l...</td>\n",
              "      <td>d lost</td>\n",
              "      <td>negative</td>\n",
              "      <td>wish we could come see u on Denver husband los...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27477</th>\n",
              "      <td>4f4c4fc327</td>\n",
              "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
              "      <td>, don`t force</td>\n",
              "      <td>negative</td>\n",
              "      <td>I`ve wondered about rake to. The client ha mad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27478</th>\n",
              "      <td>f67aae2310</td>\n",
              "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
              "      <td>Yay good for both of you.</td>\n",
              "      <td>positive</td>\n",
              "      <td>Yay good for both of you. Enjoy the break - yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27479</th>\n",
              "      <td>ed167662a5</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>positive</td>\n",
              "      <td>But it wa worth it ****.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16363 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           textID                                               text  \\\n",
              "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
              "2      088c60f138                          my boss is bullying me...   \n",
              "3      9642c003ef                     what interview! leave me alone   \n",
              "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
              "6      6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
              "...           ...                                                ...   \n",
              "27475  b78ec00df5                                     enjoy ur night   \n",
              "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
              "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
              "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
              "27479  ed167662a5                         But it was worth it  ****.   \n",
              "\n",
              "                    selected_text sentiment  \\\n",
              "1                        Sooo SAD  negative   \n",
              "2                     bullying me  negative   \n",
              "3                  leave me alone  negative   \n",
              "4                   Sons of ****,  negative   \n",
              "6                             fun  positive   \n",
              "...                           ...       ...   \n",
              "27475                       enjoy  positive   \n",
              "27476                      d lost  negative   \n",
              "27477               , don`t force  negative   \n",
              "27478   Yay good for both of you.  positive   \n",
              "27479  But it was worth it  ****.  positive   \n",
              "\n",
              "                                                   lemma  \n",
              "1          Sooo SAD I will miss you here in San Diego!!!  \n",
              "2                               my bos is bullying me...  \n",
              "3                         what interview! leave me alone  \n",
              "4      Sons of ****, why couldn`t they put them on th...  \n",
              "6      2am feeding for the baby are fun when he is al...  \n",
              "...                                                  ...  \n",
              "27475                                     enjoy ur night  \n",
              "27476  wish we could come see u on Denver husband los...  \n",
              "27477  I`ve wondered about rake to. The client ha mad...  \n",
              "27478  Yay good for both of you. Enjoy the break - yo...  \n",
              "27479                           But it wa worth it ****.  \n",
              "\n",
              "[16363 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVY5DuOHydQ0"
      },
      "source": [
        " \n",
        "6.\tCopy the lemma column into a Series X, and the sentiment column into a Series y. Apply a train test split with the random_state = 32.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSTVH2_OydQ0"
      },
      "source": [
        "X,y=tweetsdf['lemma'],tweetsdf['sentiment']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JL2et5GydQ0"
      },
      "source": [
        "7.\tCreate a vectorizer model with scikit-learn using the Countvectorizer method. Train your model on X_train, then create a matrix of features X_train_CV. Create the X_test_CV matrix without re-training the model. The format of the X_test_CV matrix should be 4091x13838 with 44274 stored elements.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqGqBklwydQ1",
        "outputId": "2ac648f8-b9bd-477a-97e3-10c7d1ffab0a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr=LogisticRegression(max_iter=1000)\n",
        "count_vectorizer = CountVectorizer()\n",
        "cvec = count_vectorizer.fit(X_train)\n",
        "X_train_cvec = cvec.transform(X_train)\n",
        "lr.fit(X_train_cvec,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHSHAytDydQ1",
        "outputId": "a61e8695-4eef-4a74-8f44-6ecf357859cb"
      },
      "source": [
        "cvec_score=cross_val_score(lr,X_train_cvec,y_train,cv=3)\n",
        "cvec_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.87362503, 0.8665363 , 0.87066015])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhElnfvydQ1",
        "outputId": "8043dd37-4504-4bed-dd5c-a1bf8863ba3d"
      },
      "source": [
        "X_train_cvec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<12272x15392 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 144148 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzSKJH2CydQ2",
        "outputId": "021a675d-1ac6-46c0-cd99-5f215c361188"
      },
      "source": [
        "X_test_CV=cvec.transform(X_test)\n",
        "X_test_CV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4091x15392 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 44554 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wquUICTgydQ2"
      },
      "source": [
        "8.\tUsing X_train_CV and y_train, please run a logistic regression to classify tweets. You should get a score of 0.87 on the X_test_CV test set.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVsNI1D3ydQ2",
        "outputId": "bf90eab1-b168-4695-9aee-4950a1e9089f"
      },
      "source": [
        "cross_val_score(lr,X_test_CV,y_test,cv=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.83870968, 0.83577713, 0.83639032])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aY-LKG9ydQ3"
      },
      "source": [
        "9.\tRepeat steps 7 and 8, adding the stop_words parameter to the CountVectorizer, using your stopwordsenglish list from step 3. Is the score very different? Is the number of elements stored in the sparse matrix very different?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD33TqyGydQ3",
        "outputId": "5dcd9743-29d7-471a-cfc4-49739f607365"
      },
      "source": [
        "\n",
        "lr1=LogisticRegression()\n",
        "count_vectorizer1 = CountVectorizer(stop_words='english')\n",
        "X_train_cvec1 = count_vectorizer1.fit_transform(X_train)\n",
        "#X_train_cvec1 = cvec1.transform(X_train)\n",
        "lr1.fit(X_train_cvec1,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFIX210nydQ3",
        "outputId": "bae19ae5-7dd3-4591-f861-f1573ffc5aaa"
      },
      "source": [
        "X_train_cvec1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<12272x15118 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 83163 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wdMixeBydQ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOHlvblHydQ4",
        "outputId": "1c0d7f8a-9f4d-40bc-87a2-e61d06637afd"
      },
      "source": [
        "X_test_CV1=cvec1.transform(X_test)\n",
        "print('Cross val score:',cross_val_score(lr1,X_train_cvec1,y_train,cv=3).mean())\n",
        "print('Accuracy score:', accuracy_score(y_train,lr1.predict(X_train_cvec1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross val score: 0.860332488853322\n",
            "Accuracy score: 0.9586049543676662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnkbUoHqydQ4",
        "outputId": "ea34626e-96be-47e3-93e6-228076c30d0c"
      },
      "source": [
        "X_test_CV1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4091x15118 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 24569 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbI7G_wydQ5"
      },
      "source": [
        "10.\tTest the model using TfidfVectorizer instead of Countvectorizer, and repeat steps 7 and 8. Which preprocessing method gives you the best results on the test set? Are the scores very different on this dataset?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0ZaKBJOydQ5",
        "outputId": "c2b0ecf6-1c30-454a-bb13-999028b83587"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "lrtfidf=LogisticRegression()\n",
        "tfidfvec = tfidf.fit(X_train)\n",
        "X_train_tfidf = tfidfvec.transform(X_train)\n",
        "lrtfidf.fit(X_train_tfidf,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkVsU4jyydQ6",
        "outputId": "627a0ddc-4d4a-4b60-f9b9-f41894b66996"
      },
      "source": [
        "print('Cross val score:', cross_val_score(lrtfidf,X_train_tfidf,y_train,cv=3).mean())\n",
        "print('Accuracy score:', accuracy_score(y_train,lrtfidf.predict(X_train_tfidf)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross val score: 0.8665255215645212\n",
            "Accuracy score: 0.9310625814863103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oSCVqa2ydQ6",
        "outputId": "c3396178-3621-4433-8a87-e8b5f93f4b7e"
      },
      "source": [
        "X_test_tfidf=tfidfvec.transform(X_test)\n",
        "print(cross_val_score(lrtfidf,X_test_tfidf,y_test,cv=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8313783  0.83064516 0.84079237]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pym3sGbSydQ6"
      },
      "source": [
        "## Using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIO-4wf7ydQ7",
        "outputId": "a50e100d-7faa-4b34-9441-fea7bf336437"
      },
      "source": [
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "pipe = Pipeline(steps=[(\"cbow\", CountVectorizer()), \n",
        "                       (\"lr\", LogisticRegression(random_state=32,max_iter=1000))])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(y_train, pipe.predict(X_train)))\n",
        "print(cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
        "print(accuracy_score(y_test, pipe.predict(X_test)))\n",
        "\n",
        "\n",
        "print(\"\\nLogloss\")\n",
        "print(log_loss(y_train, pipe.predict_proba(X_train)[:,1]))\n",
        "print(-1*cross_val_score(pipe,X_train, y_train, cv=5, scoring=\"neg_log_loss\").mean())\n",
        "print(log_loss(y_test, pipe.predict_proba(X_test)[:,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "0.9658572359843546\n",
            "0.8702734635002998\n",
            "0.8731361525299438\n",
            "\n",
            "Logloss\n",
            "0.16128167549530212\n",
            "0.31527710807440656\n",
            "0.3098536173482783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCOCF9k2ydQ7",
        "outputId": "9f4c5d80-cde6-45a9-efd7-bec1a9faf505"
      },
      "source": [
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "pipe = Pipeline(steps=[(\"tfidf\", TfidfVectorizer()), \n",
        "                       (\"lr\", LogisticRegression(random_state=32))])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(y_train, pipe.predict(X_train)))\n",
        "print(cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
        "print(accuracy_score(y_test, pipe.predict(X_test)))\n",
        "\n",
        "\n",
        "print(\"\\nLogloss\")\n",
        "print(log_loss(y_train, pipe.predict_proba(X_train)[:,1]))\n",
        "print(-1*cross_val_score(pipe,X_train, y_train, cv=5, scoring=\"neg_log_loss\").mean())\n",
        "print(log_loss(y_test, pipe.predict_proba(X_test)[:,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "0.9310625814863103\n",
            "0.8697847647217974\n",
            "0.8692251283304816\n",
            "\n",
            "Logloss\n",
            "0.30002890875497806\n",
            "0.36118196753688375\n",
            "0.3444881340684517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5a6XFDAydQ8"
      },
      "source": [
        "11.\tBonus step: try to display 10 tweets that were badly predicted (false positive or false negative). Would you have done better than the algorithm  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7nryuAydQ8",
        "outputId": "0da35143-afc4-4534-d3e8-685807ade8e2"
      },
      "source": [
        "len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgfL5V3WydQ9"
      },
      "source": [
        "predf=pd.concat([X_test,y_test],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohnVzKWTydQ9"
      },
      "source": [
        "predf['predictions']=predictions.tolist()\n",
        "#X_test\n",
        "#y_test\n",
        "\n",
        "#predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSSEYqqvydQ9",
        "outputId": "45f5019a-4177-406c-c687-9b2013c56246"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "predf[predf['predictions']!=predf['sentiment']][:19]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18732</th>\n",
              "      <td>SUFFICATION NO BREATHING. It`s okay. There`ll be more. You`re invited to mine, but I can`t promise fun times. *Jinx</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12055</th>\n",
              "      <td>i wanna vote for Miley Cyrus for the mtv movie awards..but i don`t know where i could somebody could send me a link? thaank you &lt;3</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21824</th>\n",
              "      <td>I love music so much that i`ve gone through pain to play :S my side of my finger now are peeling and have blister from playing so much</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18465</th>\n",
              "      <td>I can only message those who message me, if we`re fwends...so those that want replies..follow me. hmm..that sound funny..</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2976</th>\n",
              "      <td>wish I could feel no pain (8) but it`s ok, at least they like Brazil!</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3922</th>\n",
              "      <td>so glad i`m not at uni anymore</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5199</th>\n",
              "      <td>You`re not here. I hope you`re still resting. I don`t want you to be stressed.</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>you`re missing out, bb! i`m such a cereal nut, i think i like every kind available.</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15216</th>\n",
              "      <td>have an amazing time with your momma tomorrow! Show them how much they mean to you Whatever you do they will love it</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2977</th>\n",
              "      <td>hoping i didn`t fail english. that would just be sad</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19434</th>\n",
              "      <td>awh, thats not good, get better soon!</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6458</th>\n",
              "      <td>I don`t know what my Mom gave me to clean my Macbook with but it is SO white now. It wa tinted grayish blue from my black desk.</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10009</th>\n",
              "      <td>I don`t think I can bear such cuteness this early in the day! Kudos to Fair Empress</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25456</th>\n",
              "      <td>Oh, my God..... The end of the first course))) don`t believe in it) but, yeh, so many exam La-la-la// everything is wonderful!!!!!!!!!!</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12759</th>\n",
              "      <td>Going to school =[ I`m actually not so tired today tho</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19442</th>\n",
              "      <td>Alas, the best I can offer is a small pony and a rowing boat</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12785</th>\n",
              "      <td>super duber high! this klondike bar is thee business.</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8399</th>\n",
              "      <td>There`s nothing good on tonight anyway!! #Sigjeans</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21347</th>\n",
              "      <td>moving 11 block up the street. goodbye hardwood floor</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                         lemma  \\\n",
              "18732                      SUFFICATION NO BREATHING. It`s okay. There`ll be more. You`re invited to mine, but I can`t promise fun times. *Jinx   \n",
              "12055       i wanna vote for Miley Cyrus for the mtv movie awards..but i don`t know where i could somebody could send me a link? thaank you <3   \n",
              "21824   I love music so much that i`ve gone through pain to play :S my side of my finger now are peeling and have blister from playing so much   \n",
              "18465                I can only message those who message me, if we`re fwends...so those that want replies..follow me. hmm..that sound funny..   \n",
              "2976                                                                     wish I could feel no pain (8) but it`s ok, at least they like Brazil!   \n",
              "3922                                                                                                            so glad i`m not at uni anymore   \n",
              "5199                                                            You`re not here. I hope you`re still resting. I don`t want you to be stressed.   \n",
              "468                                                        you`re missing out, bb! i`m such a cereal nut, i think i like every kind available.   \n",
              "15216                     have an amazing time with your momma tomorrow! Show them how much they mean to you Whatever you do they will love it   \n",
              "2977                                                                                      hoping i didn`t fail english. that would just be sad   \n",
              "19434                                                                                                    awh, thats not good, get better soon!   \n",
              "6458           I don`t know what my Mom gave me to clean my Macbook with but it is SO white now. It wa tinted grayish blue from my black desk.   \n",
              "10009                                                      I don`t think I can bear such cuteness this early in the day! Kudos to Fair Empress   \n",
              "25456  Oh, my God..... The end of the first course))) don`t believe in it) but, yeh, so many exam La-la-la// everything is wonderful!!!!!!!!!!   \n",
              "12759                                                                                   Going to school =[ I`m actually not so tired today tho   \n",
              "19442                                                                             Alas, the best I can offer is a small pony and a rowing boat   \n",
              "12785                                                                                    super duber high! this klondike bar is thee business.   \n",
              "8399                                                                                        There`s nothing good on tonight anyway!! #Sigjeans   \n",
              "21347                                                                                    moving 11 block up the street. goodbye hardwood floor   \n",
              "\n",
              "      sentiment predictions  \n",
              "18732  negative    positive  \n",
              "12055  positive    negative  \n",
              "21824  negative    positive  \n",
              "18465  positive    negative  \n",
              "2976   positive    negative  \n",
              "3922   positive    negative  \n",
              "5199   positive    negative  \n",
              "468    positive    negative  \n",
              "15216  negative    positive  \n",
              "2977   positive    negative  \n",
              "19434  negative    positive  \n",
              "6458   positive    negative  \n",
              "10009  positive    negative  \n",
              "25456  positive    negative  \n",
              "12759  positive    negative  \n",
              "19442  negative    positive  \n",
              "12785  positive    negative  \n",
              "8399   negative    positive  \n",
              "21347  positive    negative  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7UC7aOrydQ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}