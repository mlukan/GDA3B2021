{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/A19893678/GDA3B2021/GDRIVE/Week7 - RNN and LSTM 2/Project/pan21-author-profiling-training-2021-03-14/en'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639b8e5e6a527d494c85d8f5704b1a01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2b1fc84c500c38a93522efbd422b559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10b2d013382e1fb3c9414ea28329f258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26644d1348fc1122e8c5ef45d6bc84fa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4d4c5dcbfe38d0d33a0d1b1419952ca8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>833ffc26e9f1a81265e3e97513cefb86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>dead14502e55f545546f666ab6d91558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>e608b622e6a3085290fc279b55fa821c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>a7d4e6f2aa8543a448b5a07feab9fc49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>41501686277ace6b5fd7dcfe9284fe1d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text class\n",
       "0    639b8e5e6a527d494c85d8f5704b1a01     0\n",
       "1    f2b1fc84c500c38a93522efbd422b559     0\n",
       "2    10b2d013382e1fb3c9414ea28329f258     0\n",
       "3    26644d1348fc1122e8c5ef45d6bc84fa     0\n",
       "4    4d4c5dcbfe38d0d33a0d1b1419952ca8     0\n",
       "..                                ...   ...\n",
       "195  833ffc26e9f1a81265e3e97513cefb86     1\n",
       "196  dead14502e55f545546f666ab6d91558     1\n",
       "197  e608b622e6a3085290fc279b55fa821c     1\n",
       "198  a7d4e6f2aa8543a448b5a07feab9fc49     1\n",
       "199  41501686277ace6b5fd7dcfe9284fe1d     1\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doclist=[]\n",
    "with open('truth.txt') as f:\n",
    "    mylist=f.readlines()\n",
    "    #print(mylist)\n",
    "    for i in mylist:\n",
    "        i.strip()\n",
    "        k=i.split(':')[0]\n",
    "        v=i.split(':')[-1].replace('\\n','')\n",
    "        doclist.append({'text':k,'class':v})\n",
    "        \n",
    "#print(docdict)\n",
    "truthdf=pd.DataFrame(doclist)\n",
    "truthdf\n",
    "#doclist\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "enlist=[]\n",
    "#Loop over all files in current directory (change directory if necessary)\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    #get current file\n",
    "    current_file = os.path.join(os.getcwd(), file)\n",
    "    #process all files having 'xml' in name\n",
    "    if 'xml' in current_file:\n",
    "        data = open(current_file, \"r\")\n",
    "        #Parse XML tree\n",
    "        tree = ET.parse(data)\n",
    "        root = tree.getroot()\n",
    "        #Get class attribute\n",
    "        cls=root.attrib['class']\n",
    "        #Find all documents in file\n",
    "        docs=root.find('documents')\n",
    "        #Append documents to the list 'texts'\n",
    "        texts = [re.sub(\"RT:|#.*#\", \"\", doc.text) for doc in docs]\n",
    "        #Create a dictionary containing file names, classes and texts\n",
    "        enlist.append({'file':file,'class':cls,'texts':texts})\n",
    "import pandas as pd\n",
    "endf=pd.DataFrame(enlist)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=endf[endf['class']=='1']['texts'][0]\n",
    "for i in x:\n",
    "    print('\\n',i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf=truthdf.merge(endf,how='inner')\n",
    "len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.142857142857146"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20000/350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yeah cause I didn't vote for hi...\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext =\"#USER# #USER# Yeah cause I didn't vote for hi...\"\n",
    "import re\n",
    "re.sub(\"#.*#\", \"\", mytext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "path='https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv'\n",
    "r = requests.get(path)\n",
    "from io import StringIO\n",
    "text=StringIO(r.text)\n",
    "#print()\n",
    "hatedf=pd.read_csv(text)\n",
    "len(hatedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateser=hatedf[hatedf['class'] != 2]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatelist=list(hateser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20620"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hatelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sixes\"',\n",
       " '@WeekndJammin',\n",
       " 'four,',\n",
       " '@Iam_KiDD_PhilA',\n",
       " 'nuffin',\n",
       " 'weaaakkk&#8221;lmfaooooooooo',\n",
       " 'come-up',\n",
       " 'exercise',\n",
       " 'trial',\n",
       " '!!!!!!!',\n",
       " '@ViVaLa_Ari',\n",
       " 'piece',\n",
       " 'http://t.co/LB274Gh&#8230;',\n",
       " '52',\n",
       " '@brandonkilexxx',\n",
       " 'dates&#8221;let',\n",
       " '\"natural\"',\n",
       " 'given',\n",
       " '\"yall',\n",
       " 'Teach',\n",
       " 'immaculate',\n",
       " 'WILLIAN',\n",
       " 'Baker',\n",
       " 'Greg',\n",
       " 'in2',\n",
       " 'stereo',\n",
       " 'LGBTQ',\n",
       " 'Tierra',\n",
       " 'sons!',\n",
       " 'dontouch',\n",
       " '@OMGitsKariC:',\n",
       " '@Bargamentoo:',\n",
       " '\\n-Spanish',\n",
       " 'mead',\n",
       " 'false',\n",
       " 'brewing',\n",
       " 'Pennsylvanians.',\n",
       " 'spoopy',\n",
       " 'outta',\n",
       " 'knew',\n",
       " 'size',\n",
       " '&#128528;&#128528;&#128528;&#128528;&#128528;',\n",
       " '@axelandrewsss:',\n",
       " '#ThankYouPaulForConfirmingLarry',\n",
       " 'car.',\n",
       " 'niggad',\n",
       " 'pussy!',\n",
       " 'Juco',\n",
       " 'Cassie',\n",
       " '@EmmanuelRodz11',\n",
       " 'me??!?',\n",
       " 'Twitty.',\n",
       " 'Yaaaaaas',\n",
       " 'refused',\n",
       " 'matters',\n",
       " '@Marco_Spatula',\n",
       " 'bypass,',\n",
       " '#jealousy',\n",
       " 'watch&#8221;',\n",
       " 'connects&#128074;',\n",
       " 'visit',\n",
       " 'zactly.',\n",
       " 'paper&#128514;',\n",
       " '@chaserojo:',\n",
       " 'NAH!',\n",
       " 'http://t.co/Upie2WpJ0b',\n",
       " '&#8220;@trillasfvck:',\n",
       " 'hoovers',\n",
       " 'asses..',\n",
       " '#KingLute',\n",
       " 'Mountain',\n",
       " 'soul,',\n",
       " 'night.',\n",
       " 'College',\n",
       " 'Arguing',\n",
       " 'sister&#128514;&#128149;&#10084;&#65039;',\n",
       " '@black_taco_:',\n",
       " '@snoopDogg',\n",
       " \"'revenge.'\",\n",
       " '@_TheBlackNerd',\n",
       " 'severely,',\n",
       " 'blessing.',\n",
       " '@FuckTraVonn:',\n",
       " 'forever,',\n",
       " 'ye',\n",
       " 'Leftwich',\n",
       " '@brandonescaler',\n",
       " 'NITC',\n",
       " 'http://t.co/3UkKeyznz8',\n",
       " 'sockin',\n",
       " 'irish',\n",
       " '1&#8230;',\n",
       " '@taylerhatesyou',\n",
       " 'AHAHAHAA',\n",
       " 'grounded',\n",
       " 'Wanted',\n",
       " '@I_FloodsYaTL:',\n",
       " 'fitteds',\n",
       " 'fags&#128567;&#128567;',\n",
       " '&#128557;&#128557;&#128557;']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatewords=[]\n",
    "for i in hatelist:\n",
    "    hatewords.extend(i.split(' '))\n",
    "hateset=set(hatewords)\n",
    "hatelist= list(hateset)\n",
    "hatelist[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  string\n",
    "mypunct=string.punctuation.replace('*',\"\")\n",
    "cleaned=[]\n",
    "for word in hatelist:\n",
    "    # uncomment these for debugging to make sure it works\n",
    "    # print \"subwords\", [w for w in subwords(word)]\n",
    "    # print \"valid subwords\", [w for w in subwords(word) if w in valid]\n",
    "    if not any(w in mypunct for w in word):\n",
    "        cleaned.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cleaned,open('cleaned.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
