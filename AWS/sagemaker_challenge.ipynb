{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "sagemaker_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlukan/GDA3B2021/blob/main/AWS/sagemaker_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1mEP8or0Y1_"
      },
      "source": [
        "# <u>Introduction to Using SageMaker with Boto3</u>\n",
        "Using the previously-used Titanic dataset (which is a binary classification problem regarding survivability), we're going to use XGBoost to train our model and output predictions.\n",
        "\n",
        "To be specific, here are the steps that we're going to take during this challenge:\n",
        "- Datasets\n",
        "    - Locally download the training/validation/test datasets\n",
        "    - Upload the dataset to a newly-created S3 bucket\n",
        "- Training\n",
        "    - Use a previously-built, AWS XGBoost model for training\n",
        "    - Create a SageMaker session\n",
        "    - Train the XGBoost model from the data you created in S3\n",
        "    - Host the model on an endpoint (very basic!)\n",
        "- Predictions\n",
        "    - Make a prediction \n",
        "    - Put the prediction onto S3\n",
        "- Delete the endpoint\n",
        "- Delete your S3 Bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWImUUXT0Y2A"
      },
      "source": [
        "## <u>Datasets</u>\n",
        "\n",
        "Before we could actually start using SageMaker, the first thing that we're going to do is to download the datasets locally and then upload these datasets to a brand new S3 Bucket so that our SageMaker algorithm could easily read the datasets.\n",
        "\n",
        "You can download the datasets here: https://www.kaggle.com/c/titanic/data. When you download the .zip file locally, you should have a \"train.csv\" and a \"test.csv\" file that we're going to be using quiet extensively.\n",
        "\n",
        "Before we upload these datasets to a newly-created S3 bucket, we need to make sure the dataset is formatted in a way that the AWS XGBoost model understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CV9PJh0OL7Y"
      },
      "source": [
        "# Do the usual data preparation (i.e. get rid of unique features, clean the data and deal with missing values, and finally, encode categorical features)\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvfOlzXzO9eo"
      },
      "source": [
        "After your cleaning, you need to write the data back to train.csv and test.csv in the correct format. You can check the Input/Output Interface section on the AWS XGBoost documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
        "\n",
        "Most importantly, the algorithm assumes the target to be in the first column and that only the values of the matrix are represented (drop header and index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6CEAU49O11w"
      },
      "source": [
        "# Format and save the data to test.csv and train.csv\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixUP0ND70Y2A"
      },
      "source": [
        "# Create a new S3 bucket\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ec6sXJ50Y2E"
      },
      "source": [
        "# Upload the two files (train.csv and test.csv) to your newly-created S3 bucket\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQBy9K60Y2H"
      },
      "source": [
        "## <u>Training</u>\n",
        "\n",
        "When using SageMaker, one thing that's extremely helpful is its repository of machine learning models that you can use instantly that were created by AWS Developers in addition to the AWS community. These models include anything from what you're learning right now (LinearRegression, Random Forests, XGBoost) to more complicated, deep-learning models that are used for Object-Detection, Natural Language Processing tasks, or Reinforcement Learning. This repository can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html. \n",
        "\n",
        "To use these algorithms, all that you need to do is specify the image URI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyISJBd-0Y2I"
      },
      "source": [
        "# Use a previously-built, AWS XGBoost model for training\n",
        "\n",
        "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
        "# container = get_image_uri(***insert correct arguments here***) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6hNAvd80Y2M"
      },
      "source": [
        "# Create pointers to the S3 train and test datasets\n",
        "\n",
        "s3_input_train = # input sagemaker s3_input function here with a link to your training dataset\n",
        "s3_input_test = # input sagemaker s3_input function here with a link to your testing dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41vUL1dc0Y2P"
      },
      "source": [
        "# Create a SageMaker Session\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kLcZeRk0Y2S"
      },
      "source": [
        "# Create an XGBoost Estimator\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZsQ4OOM0Y2U"
      },
      "source": [
        "# Select the your specific hyperparameters (Optional)\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIzGexY30Y2Y"
      },
      "source": [
        "# Fit the model\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE1UnvUC0Y2b"
      },
      "source": [
        "# Deploy your model to an endpoint to perform predictions\n",
        "\n",
        "xgb_predictor = name_of_your_model.deploy(\n",
        "    initial_instance_count = 1, \n",
        "    instance_type = 'ml.t2.medium')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNo8WDa1OAJ"
      },
      "source": [
        "# Configure the predictor's serializer and deserializer\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiW76cne0Y2d"
      },
      "source": [
        "## <u>Predictions</u>\n",
        "\n",
        "Now that we have a running endpoint, we're going to use this endpoint to make predictions and then upload these predictions to S3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZ2uOeu0Y2e"
      },
      "source": [
        "# Make a prediction using xgb_predictor\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP4BEcCL0Y2h"
      },
      "source": [
        "# Upload the predicitons onto S3\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1dsA6QM0Y2k"
      },
      "source": [
        "# Delete the endpoint\n",
        "\n",
        "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwfof6w0Y2m"
      },
      "source": [
        "# Delete your S3 bucket\n",
        "\n",
        "# INSERT CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lTkExJI0Y2p"
      },
      "source": [
        "## <u>Overview</u>\n",
        "Now that you've (hopefully) successfully created a SageMaker session to run an experiment, there's several other things that you could try in order to get a better understanding of the powerful capabilities of SageMaker. In particular, if you found this challenge to be too easy, feel free to try to do the folowing:\n",
        "- Rather than using a pre-built XGBoost algorithm, use your own algorithm. For example, you can still use XGBoost as your \"own algorithm\" but you could treat this as a custom algorithm rather than using the pre-built image.\n",
        "- Hyperparameter tuning for the XGBoost hyperparameters"
      ]
    }
  ]
}