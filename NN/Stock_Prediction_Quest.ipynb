{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Prediction_Quest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlukan/GDA3B2021/blob/main/NN/Stock_Prediction_Quest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otl-MXUqPWAs"
      },
      "source": [
        "### Introduction \n",
        "In this exercise you will work with stock market data. We use the data available at [Yahoo Finance](https://finance.yahoo.com/quote/DAX/history?period1=1478390400&period2=1604534400&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true)\n",
        "You will build sequential models to predict the variations for Global X DAX Germany ETF (DAX). The goal is to predict whether the stock value is higher or lower than its actual value of previous day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHfBtj1tloYy"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9DlO6aW6fOB"
      },
      "source": [
        "### Loading and Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY8H6OBu6Z-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcce7b48-600b-4b19-ddae-ce55872f5f74"
      },
      "source": [
        "# Loading training data\n",
        "training_data = pd.read_csv(\"dax_train_2019.csv.txt\", sep=\",\")\n",
        "print(f\"Training data column info:\\n {training_data.dtypes}\")\n",
        "print(f\"Training data shape is {training_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data column info:\n",
            " date          object\n",
            "open         float64\n",
            "high         float64\n",
            "low          float64\n",
            "close        float64\n",
            "adj_close    float64\n",
            "volume         int64\n",
            "dtype: object\n",
            "Training data shape is (644, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpILl8NTZ2SW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c73a06-fa34-4b6d-e9ca-2efb2cb5e122"
      },
      "source": [
        "# Loading testing data\n",
        "testing_data = pd.read_csv(\"dax_test_2019.csv.txt\", sep=\",\")\n",
        "print(f\"Testing data column info:\\n {testing_data.dtypes}\")\n",
        "print(f\"Testing data shape is {testing_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing data column info:\n",
            " date          object\n",
            "open         float64\n",
            "high         float64\n",
            "Low          float64\n",
            "close        float64\n",
            "adj_close    float64\n",
            "volume         int64\n",
            "dtype: object\n",
            "Testing data shape is (148, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKR-Y1L2-Bz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6c3838-7eff-4bea-81fd-648616f521dd"
      },
      "source": [
        "len(testing_data)\n",
        "testing_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEGwqlyPyku7"
      },
      "source": [
        "####  Are there any NaN values in training and testing data sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muZCsfzYYXiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18da8629-f6b7-46c9-a572-30cd4f2cde20"
      },
      "source": [
        "\n",
        "training_count_nan = training_data.shape[0] - training_data.count()\n",
        "testing_count_nan = testing_data.shape[0] - testing_data.count()\n",
        "print(f\"Number of NaNs in training data columns:\\n{training_count_nan}\")\n",
        "print(f\"Number of NaNs in testing data columns:\\n{testing_count_nan}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of NaNs in training data columns:\n",
            "date         0\n",
            "open         0\n",
            "high         0\n",
            "low          0\n",
            "close        0\n",
            "adj_close    0\n",
            "volume       0\n",
            "dtype: int64\n",
            "Number of NaNs in testing data columns:\n",
            "date         0\n",
            "open         0\n",
            "high         0\n",
            "Low          0\n",
            "close        0\n",
            "adj_close    0\n",
            "volume       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4XpSecTv3u0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMDCQsRH0xsV"
      },
      "source": [
        "#### Preprocessing the data, extracting the features, and normalising the features using `preprocessing.MinMaxScaler` function from sklearn\n",
        "\n",
        "We have provided you with the relevant function here. But we expect you to explore the data generated. \n",
        "\n",
        "What are the shapes?\n",
        "\n",
        "Do a test whether features and labels correspond, i.e. whether the label for each sequence is the value following that sequence? \n",
        "\n",
        "Optional: Extend the function to include more features - and change it \n",
        "to choose a different column as target, i.e. adj_close\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqrdEVIiFPWN"
      },
      "source": [
        "# Limiting the training and testing data to \"open\" column \n",
        "training_data_processed = training_data.iloc[:, 1:2].values\n",
        "testing_data_processed = testing_data.iloc[:, 1:2].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8bFN1AvjXA1"
      },
      "source": [
        "def create_features_labels(training_data, testing_data, scaler=MinMaxScaler(feature_range = (0, 1)), sequence_length=60):\n",
        "    \"\"\"\n",
        "    Create feature and labels from training data\n",
        "    :param training_data\n",
        "    :param testing_data\n",
        "    :param sequence_length\n",
        "    :param scaler\n",
        "    :return: Features and labels sets, and labels bins\n",
        "    \"\"\"\n",
        "    features_set = []\n",
        "    labels = []\n",
        "    labels_bin = []\n",
        "    test_features = []\n",
        "    # Limiting the training and testing data to relevant column (here \"open\" column)\n",
        "    training_data_processed = training_data.iloc[:, 1:2].values\n",
        "    # Normalization: Transform features by scaling each feature in training to be in a range of (0, 1)\n",
        "    training_data_processed = scaler.fit_transform(training_data_processed)\n",
        "    # Preparing training data features and labels\n",
        "    for i in range(sequence_length, len(training_data)):\n",
        "        features_set.append(training_data_processed[i-sequence_length:i, 0])\n",
        "        labels.append(training_data_processed[i, 0])\n",
        "        # Fill binary labels\n",
        "        if training_data_processed[i,0] > training_data_processed[i-1,0]:\n",
        "            labels_bin.append(1)\n",
        "        else:\n",
        "            labels_bin.append(0)\n",
        "    # Preparing the testing data features\n",
        "    total = pd.concat((training_data['open'], testing_data['open']), axis=0)\n",
        "    test_inputs = total[len(total) - len(testing_data) - sequence_length:].values\n",
        "    test_inputs = test_inputs.reshape(-1,1)\n",
        "    test_inputs = scaler.transform(test_inputs)\n",
        "\n",
        "    for i in range(sequence_length, len(test_inputs)):\n",
        "        test_features.append(test_inputs[i-sequence_length:i, 0])\n",
        "    \n",
        "    test_features = np.array(test_features)\n",
        "    test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n",
        "    features_set, labels, labels_bin = np.array(features_set), np.array(labels), np.array(labels_bin)\n",
        "    return np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1)), labels, labels_bin, test_features, scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVe5xtf5A_Mf"
      },
      "source": [
        "features_set, labels, labels_bin, test_features, scaler = create_features_labels(training_data, testing_data, MinMaxScaler(feature_range = (0, 1)), 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffn2FwPywslN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSehb1CvxCNm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXPt3xbzTOfz"
      },
      "source": [
        "### Model\n",
        "In this section we ask you to build and train a LSTM neural network and use it predict the stock price variations on your testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hJ3Y27DTPtk"
      },
      "source": [
        "def lstm_model(input_shape):\n",
        "    # TODO: Implement your model here. \n",
        "    # (Hint: Make use of `Sequential` function from tensorflow.keras to group a linear stack of layers into the model. In addition, \n",
        "    # you can apply droupouts to your input using `Dropout` from tensorflow.keras.layers)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzAwhQyUzPa7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AYnYAORzPfY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1hqgDfwzP9X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOOzJMNVUPf6"
      },
      "source": [
        "Compile your model, train it and evaluate it. \n",
        "\n",
        "What is your mse - and what is your accuracy (note that you have two different types of labels readily available to you). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9-MZwszQrz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZd98OTBzQ4R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpnWTNN4zQ8J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiTrCOSqzRrE"
      },
      "source": [
        "Optional: Try your model with more features, try with different sequence lengths (this is a hyperparameter) - and try with the \"real\" Dax Performance Index data also provided (which reaches back longer). If you do so, you will have to split the data into training and test sets first). "
      ]
    }
  ]
}